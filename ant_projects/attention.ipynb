{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention/Transformer Implementation\n",
    "1. Core Implementation:\n",
    "   - Implement QKV attention mechanism\n",
    "   - Create basic transformer architecture\n",
    "   - Handle attention masking\n",
    "   - Implement attention-free transformers and explain what they do \n",
    "\n",
    "2. Follow-up Questions:\n",
    "   - How would you optimize memory usage?\n",
    "   - What are the tradeoffs in different attention mechanisms?\n",
    "   - How would you handle different sequence lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model  # embedding dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # linear layers for q,k,v projections and output\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # scaling factor for dot product attention\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # project and reshape q,k,v to split into heads\n",
    "        q = self.q_proj(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # attention weights\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # compute weighted values\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    # compute attention scores\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # compute attention weights and weighted sum\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFreeTransformer(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # main model dimensions\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # linear projections instead of attention\n",
    "        self.input_proj = nn.Linear(d_model, d_model)\n",
    "        self.pos_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "        # layer norm and dropout\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encoding\n",
    "        self.pos_encoding = self._create_positional_encoding()\n",
    "\n",
    "    def _create_positional_encoding(self, max_len=5000):\n",
    "        # create sinusoidal positional encodings\n",
    "        pe = torch.zeros(max_len, self.d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        b, l, d = x.shape\n",
    "\n",
    "        # add positional encoding\n",
    "        pos = self.pos_encoding[:, :l, :].to(x.device)\n",
    "        x = x + pos\n",
    "\n",
    "        # first sublayer: linear projection instead of attention\n",
    "        residual = x\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        # second sublayer: feedforward network\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SimpleAttentionFreeTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        # embedding layer converts tokens to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # main transformer layer\n",
    "        self.transformer = AttentionFreeTransformer(d_model=d_model)\n",
    "\n",
    "        # output projection\n",
    "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert tokens to embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # pass through transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # project to vocabulary size\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
